[agent]
  interval = "1m"
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "0s"
  flush_interval = "10s"
  flush_jitter = "0s"
  precision = ""
  debug = false
  quiet = true
  logfile = ""
  hostname = "$REPORTED_HOSTNAME"
  omit_hostname = false

  
# Monitor redis
# AND retrieve the sidekiq stats from it
[[inputs.redis]]
  servers = ["tcp://redis:6379"]
  
  [[inputs.redis.commands]]
      command = ["get", "stat:processed"]
      field = "sidekiq_stat_processed"
      type = "integer"
  
  [[inputs.redis.commands]]
      command = ["get", "stat:failed"]
      field = "sidekiq_stat_failed"
      type = "integer"

  
  [[inputs.redis.commands]]
      command = ["llen", "queue:#default"]
      field = "sidekiq_default_queue_len"
      type = "integer"

  [[inputs.redis.commands]]
      command = ["llen", "queue:#ingress"]
      field = "sidekiq_ingress_queue_len"
      type = "integer"

  [[inputs.redis.commands]]
      command = ["llen", "queue:#mailers"]
      field = "sidekiq_mailers_queue_len"
      type = "integer"

  [[inputs.redis.commands]]
      command = ["llen", "queue:#pull"]
      field = "sidekiq_pull_queue_len"
      type = "integer"      
      
  [[inputs.redis.commands]]
      command = ["llen", "queue:#push"]
      field = "sidekiq_push_queue_len"
      type = "integer"      
      
  [[inputs.redis.commands]]
      command = ["llen", "queue:#scheduler"]
      field = "sidekiq_scheduler_queue_len"
      type = "integer"  
  
  [[inputs.redis.commands]]
      command = ["zcard", "schedule"]
      field = "sidekiq_scheduled"
      type = "integer"  
      
  [[inputs.redis.commands]]
      command = ["zcard", "retry"]
      field = "sidekiq_retries"
      type = "integer"  
          
  [[inputs.redis.commands]]
      command = ["zcard", "dead"]
      field = "sidekiq_dead"
      type = "integer"  
                
  [[inputs.redis.commands]]
      command = ["scard", "processes"]
      field = "sidekiq_processes"
      type = "integer"          

  
  
  
# Monitor Postgres
[[inputs.postgresql]]
    address = "host=db user=postgres sslmode=disable"
    # give it a nice name
    outputaddress = "postgres-db01"

    # Set this to false if PgBouncer is in use with
    # pool_mode set to transaction
    # prepared_statements = true

    
# Send HTTP probes to check that the service is responding
[[inputs.http_response]]
    urls = ["http://web:3000",
            "https://$MASTODON_DOMAIN"
            ]
   
    response_timeout = "5s"
    method = "GET"
    response_status_code = 200
    
    [inputs.http_response.headers]
      # CHANGEME: this must match your mastodon domain name
      Host = "$MASTODON_DOMAIN"
      X-Forwarded-Proto = "https"
      

# Check the streaming container, we expect a 401 because we're not including a valid token
[[inputs.http_response]]
    # Provide your eepsite URLs here
    urls = ["http://streaming:4000"]
   
    response_timeout = "5s"
    method = "GET"
    response_status_code = 401
    
    [inputs.http_response.headers]
      # CHANGEME: this must match your mastodon domain name
      Host = "$MASTODON_DOMAIN"
      X-Forwarded-Proto = "https"
      
# Check the TLS certificate being served
[[inputs.x509_cert]]
    sources = ["tcp://$MASTODON_DOMAIN:443"]
    server_name = "$MASTODON_DOMAIN"

[[outputs.influxdb_v2]]
  ## The URLs of the InfluxDB cluster nodes.
  ##
  ## Multiple URLs can be specified for a single cluster, only ONE of the
  ## urls will be written to each interval.
  ##   ex: urls = ["https://us-west-2-1.aws.cloud2.influxdata.com"]
  urls = ["$INFLUXDB_URL"]
  token = "$INFLUXDB_TOKEN"
  organization = "$INFLUXDB_ORG"
  bucket = "$INFLUXDB_BUCKET"

